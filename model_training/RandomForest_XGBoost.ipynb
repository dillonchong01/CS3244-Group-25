{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqTwqbok3eVY"
      },
      "source": [
        "# **Import Required Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WeSxZvgc086L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRaP28-83i2p"
      },
      "source": [
        "# **Load and Preprocess Data**\n",
        "We drop the 'Town' and 'Address' columns, before splitting the dataset into train-test and applying one-hot encoding to categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkzSB0Ps3ikM"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "df = pd.read_csv('../datasets/Final_ResaleData.csv')\n",
        "df = df.drop(columns=['Town', 'Address'])\n",
        "\n",
        "# Split Dataset into Train (80%) and Test (20%) Sets\n",
        "X = df.drop(columns=['Price'])\n",
        "y = df['Price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing pipeline for categorical features (One-Hot Encoding)\n",
        "categorical_columns = [\"Flat_Type\"]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBLB6BQ54XPf"
      },
      "source": [
        "# **Train Random Forest Model**\n",
        "A pipeline is built for preprocessing and a Random Forest regressor. Hyperparameter fine-tuning is then carried out using HalvingGridSearchCV, which efficiently narrows down the best combination of parameters (like tree depth, number of estimators, and split criteria) by progressively focusing on the most promising configurations.\n",
        "\n",
        "After finetuning, here are the best hyperparameters:\n",
        "- 'model__max_depth': None\n",
        "- 'model__min_samples_leaf': 2\n",
        "- 'model__min_samples_split': 2\n",
        "- 'model__n_estimators': 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0FAJoJ74aAU",
        "outputId": "aa35c696-44b8-4c74-93c3-91c041238d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_iterations: 5\n",
            "n_required_iterations: 5\n",
            "n_possible_iterations: 5\n",
            "min_resources_: 10847\n",
            "max_resources_: 173556\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 24\n",
            "n_resources: 10847\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 12\n",
            "n_resources: 21694\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 6\n",
            "n_resources: 43388\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 3\n",
            "n_resources: 86776\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "----------\n",
            "iter: 4\n",
            "n_candidates: 2\n",
            "n_resources: 173552\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
            "Best Random Forest Parameters: {'model__max_depth': None, 'model__min_samples_leaf': 2, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n"
          ]
        }
      ],
      "source": [
        "# Pipeline for Random Forest\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter Tuning for Random Forest\n",
        "rf_params = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [10, 20, None],\n",
        "    'model__min_samples_split': [2, 5],\n",
        "    'model__min_samples_leaf': [2, 4],\n",
        "}\n",
        "\n",
        "# GridSearchCV for Random Forest\n",
        "rf_grid_search = HalvingGridSearchCV(estimator=rf_pipeline, param_grid=rf_params, cv=3, factor=2, scoring='neg_mean_squared_error', verbose=1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters for Random Forest\n",
        "best_rf_params = rf_grid_search.best_params_\n",
        "print(f'Best Random Forest Parameters: {best_rf_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYSUC6pL4fy_"
      },
      "source": [
        "# **Evaluate Random Forest Model**\n",
        "Evaluation is carried out using RMSE (38243.07), MAE (25831.06) and R^2 (0.95665) to assess prediction accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apdgjVHK4kkD",
        "outputId": "50e5a29f-f1b1-4f33-c409-355980ab04ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest RMSE: 38243.07451691749\n",
            "Random Forest MAE: 25831.06009914324\n",
            "Random Forest R^2: 0.956652309835798\n"
          ]
        }
      ],
      "source": [
        "rf_best_model = rf_grid_search.best_estimator_\n",
        "y_pred_rf = rf_best_model.predict(X_test)\n",
        "\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest RMSE: {rmse_rf}\")\n",
        "print(f\"Random Forest MAE: {mae_rf}\")\n",
        "print(f\"Random Forest R^2: {r2_rf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lb9N-tp4pHD"
      },
      "source": [
        "# **Train XGBoost Model**\n",
        "A pipeline is built for preprocessing and XGBRegressor. Hyperparameter fine-tuning is then carried out using GridSearchCV, which efficiently narrows down the best combination of parameters (like tree depth, number of estimators, learning rate, and subsampling ratios)\n",
        "\n",
        "After finetuning, here are the best hyperparameters:\n",
        "- 'model__colsample_bytree': 1.0\n",
        "- 'model__learning_rate': 0.1\n",
        "- 'model__max_depth': 10\n",
        "- 'model__n_estimators': 250\n",
        "- 'model__subsample': 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6723gofx4wDG",
        "outputId": "43e69c61-3d3b-4de6-a3ab-50529ef76be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
            "Best XGBoost Parameters: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.1, 'model__max_depth': 10, 'model__n_estimators': 250, 'model__subsample': 0.8}\n"
          ]
        }
      ],
      "source": [
        "# Pipeline for XGBoost\n",
        "xgb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', XGBRegressor(tree_method='hist', random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter Tuning for XGBoost\n",
        "xgb_params = {\n",
        "    'model__n_estimators': [100, 200, 250],\n",
        "    'model__max_depth': [5, 10, 15],\n",
        "    'model__learning_rate': [0.05, 0.1],\n",
        "    'model__subsample': [0.8, 1.0],\n",
        "    'model__colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# GridSearchCV for XGBoost\n",
        "xgb_grid_search = GridSearchCV(estimator=xgb_pipeline, param_grid=xgb_params, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters for XGBoost\n",
        "best_xgb_params = xgb_grid_search.best_params_\n",
        "print(f'Best XGBoost Parameters: {best_xgb_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50eUNces43nh"
      },
      "source": [
        "# **Evaluate XGBoost Model**\n",
        "Evaluation is carried out using RMSE (32223.05), MAE (22707.33) and R^2 (0.96923) to assess prediction accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORD8tUga43Q5",
        "outputId": "7f910651-bc1a-4017-f52b-6a7e47beb954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost RMSE: 32223.0457240883\n",
            "XGBoost MAE: 22707.330887877393\n",
            "XGBoost R^2: 0.9692253206776796\n"
          ]
        }
      ],
      "source": [
        "xgb_best_model = xgb_grid_search.best_estimator_\n",
        "y_pred_xgb = xgb_best_model.predict(X_test)\n",
        "\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost RMSE: {rmse_xgb}\")\n",
        "print(f\"XGBoost MAE: {mae_xgb}\")\n",
        "print(f\"XGBoost R^2: {r2_xgb}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
